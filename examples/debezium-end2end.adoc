# Title: Real-time Database Streaming with Debezium and AMQ Streams
Demonstrating the integration of Debezium and AMQ Streams for real-time database streaming using MySQL 8.0.

Environment Setup:

- AMQ Streams 2.5.0
- Debezium 2.4.0.Final
- MySQL 8.0

Prerequisites:

- Install AMQ Streams Operator from OperatorHub
- Install CamelK Operator from OperatorHub
- Install Service Registry Operator 


## 1. Kafka Cluster Creation

Create the 'dbz-mysql' namespace: `oc new-project dbz-mysql`

Create Kafka clusters using Kafka CR YAML configuration

[source, yaml,indent=0]
----
    oc create -f - <<EOF
    apiVersion: kafka.strimzi.io/v1beta2
    kind: Kafka
    metadata:
      name: my-cluster
    spec:
      kafka:
        config:
          offsets.topic.replication.factor: 1
          transaction.state.log.replication.factor: 1
          transaction.state.log.min.isr: 2
          default.replication.factor: 1
          min.insync.replicas: 1
          inter.broker.protocol.version: '3.5'
        storage:
          type: jbod
          volumes:
            - deleteClaim: false
              id: 0
              size: 1Gi
              type: persistent-claim
        listeners:
          - name: plain
            port: 9092
            type: internal
            tls: false
          - name: tls
            port: 9093
            type: internal
            tls: true
        version: 3.5.0
        replicas: 3
      entityOperator:
        topicOperator: {}
        userOperator: {}
      zookeeper:
        storage:
          deleteClaim: false
          size: 1Gi
          type: persistent-claim
        replicas: 3
    EOF
----

## 2. Install Kafdrop:

[source, yaml,indent=0]
----
oc apply -f https://raw.githubusercontent.com/aboucham/debezium/main/examples/kafdrop.yaml
----

## 3. Depploy ApiCurio Registry:

Deploy the postgres database
Deploy the ApicurioRegistry CR
[source, yaml,indent=0]
----
oc new-app -e POSTGRESQL_USER=psql -e POSTGRESQL_PASSWORD=password -e POSTGRESQL_DATABASE=apicurio postgresql
----
[source, yaml,indent=0]
----
oc create -f - <<EOF
apiVersion: registry.apicur.io/v1
kind: ApicurioRegistry
metadata:
  name: apicurioregistry-psql
spec:
  configuration:
    env:
      - name: REGISTRY_AUTH_RBAC_ENABLED
        value: 'true'
    persistence: "sql"
    sql:
      dataSource:
        url: "jdbc:postgresql://postgresql:5432/apicurio"
        userName: "psql"
        password: "password"
EOF
----


## 4. Build and deploy Debezium connectors using KafkaConnect Custom Resource (CR)

- Install Kafka connect CR with Debezium plugins through AMQ Streams:


[source, yaml,indent=0]
----
oc create -f - <<EOF
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnect
metadata:
  name: debezium-connect
  annotations:
    strimzi.io/use-connector-resources: "true"
spec:
  bootstrapServers: 'my-cluster-kafka-bootstrap:9093'
  build:
    output:
      image: >-
        image-registry.openshift-image-registry.svc:5000/dbz-mysql/dbz-mysql-connect:latest
      type: docker
    plugins:
      - artifacts:
          - type: tgz
            url: >-
              https://repo1.maven.org/maven2/io/debezium/debezium-connector-mongodb/2.4.0.Final/debezium-connector-mongodb-2.4.0.Final-plugin.tar.gz
        name: debezium-mongodb-connector
      - artifacts:
          - type: tgz
            url: >-
              https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/2.4.0.Final/debezium-connector-postgres-2.4.0.Final-plugin.tar.gz
        name: debezium-postgres-connector
      - artifacts:
          - type: tgz
            url: >-
              https://repo1.maven.org/maven2/io/debezium/debezium-connector-mysql/2.4.0.Final/debezium-connector-mysql-2.4.0.Final-plugin.tar.gz
        name: debezium-mysql-connector
      - artifacts:
          - type: tgz
            url: >-
              https://repo1.maven.org/maven2/io/apicurio/apicurio-registry-distro-connect-converter/2.4.2.Final/apicurio-registry-distro-connect-converter-2.4.2.Final.tar.gz
        name: apicurio-registry-distro-connect-converter
  config:
    config.storage.replication.factor: -1
    config.storage.topic: debezium-connect-configs
    group.id: debezium-connect-cluster
    offset.storage.replication.factor: -1
    offset.storage.topic: debezium-connect-offsets
    status.storage.replication.factor: -1
    status.storage.topic: debezium-connect-status
  replicas: 1
  tls:
    trustedCertificates:
      - certificate: ca.crt
        secretName: my-cluster-cluster-ca-cert
  version: 3.5.0
EOF
----

TIP: Enable `use-connector-resources` to instantiate Kafka connectors through specific custom resources:
`oc annotate kafkaconnects2is my-connect-cluster strimzi.io/use-connector-resources=true`

NOTE: Multiple instances attempting to use the same internal topics will cause unexpected errors, so you must change the values of these properties for each instance.


### Check

[source, yaml,indent=0]
----
oc get kc debezium-connect -o yaml | yq '.status.connectorPlugins'
----

## 3. Deploy pre-populated MySQL instance

#### Configure credentials for the database:

[source, yaml,indent=0]
----
oc new-app \
    -e MYSQL_ROOT_PASSWORD=debezium \
    -e MYSQL_USER=mysqluser \
    -e MYSQL_PASSWORD=mysqlpw \
    -e MYSQL_DATABASE=inventory \
    mysql:8.0-el9
----

[source, yaml,indent=0]
----
oc rsh `oc get pods -l deployment=mysql -o name` mysql -u mysqluser -pmysqlpw inventory
----
[source, yaml,indent=0]
----
CREATE TABLE products
(
    id INT PRIMARY KEY NOT NULL,
    name VARCHAR(100),
    model VARCHAR(100),
    price INT
);


INSERT INTO products VALUES (1, 'LenovoT41', 'Lenovo T 41', 3);
INSERT INTO products VALUES (2, 'LenovoT41', 'Lenovo UT 41', 45);
INSERT INTO products VALUES (3, 'DELL', 'DELL 41', 45);
----

## - Kafka Connector CR: Create KC with MYSQL Connector:

[source, yaml,indent=0]
----
oc create -f - <<EOF
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnector
metadata:
  labels:
    strimzi.io/cluster: debezium-connect
  name: mysql-connector
spec:
  class: io.debezium.connector.mysql.MySqlConnector
  tasksMax: 1
  autoRestart:
    enabled: true
  config:
    database.hostname: mysql
    database.port: 3306
    database.user: root
    database.password: debezium
    database.server.id: 184057
    database.whitelist: inventory
    database.names: inventory
    include.schema.changes: false
    schema.history.internal.kafka.topic: schemahistory.fullfillment
    schema.history.internal.kafka.bootstrap.servers: 'my-cluster-kafka-bootstrap:9092'
    topic.prefix: mysql
    topic.creation.default.replication.factor: 1
    topic.creation.default.partitions: 1
EOF
----

#### Check Status:

[source, yaml,indent=0]
----
$ oc get kctr
NAME              CLUSTER             CONNECTOR CLASS                              MAX TASKS   READY
mysql-connector   dbz-mysql-connect   io.debezium.connector.mysql.MySqlConnector   1           True
----

[source, yaml,indent=0]
----
oc get kctr mysql-connector -o yaml | yq '.status'
----

[source, yaml,indent=0]
----
status:
  conditions:
  - lastTransitionTime: "2023-10-24T12:12:59.267139132Z"
    status: "True"
    type: Ready
  connectorStatus:
    connector:
      state: RUNNING
      worker_id: 10.131.0.22:8083
    name: mysql-connector
    tasks:
    - id: 0
      state: RUNNING
      worker_id: 10.131.0.22:8083
    type: source
  observedGeneration: 1
  tasksMax: 1
  topics:
  - mysql.inventory.products
----

## Add signal configuration and trigger ad hoc snapshot:

### Signal ;;;

[source, yaml,indent=0]
----
oc rsh `oc get pods -l deployment=mysql -o name` mysql -u mysqluser -pmysqlpw inventory
----

The following example shows a CREATE TABLE command that creates a three-column debezium_signal table:

[source, yaml,indent=0]
----
CREATE TABLE debezium_signal (id VARCHAR(42) PRIMARY KEY, type VARCHAR(32) NOT NULL, data VARCHAR(2048) NULL);

oc rsh `oc get pods -l deployment=mysql -o name` mysql -u mysqluser -pmysqlpw inventory

CREATE TABLE customers (
  id SERIAL,
  first_name VARCHAR(255) NOT NULL,
  last_name VARCHAR(255) NOT NULL,
  email VARCHAR(255) NOT NULL,
  PRIMARY KEY(id)
);


INSERT INTO customers VALUES (1, 'Test1', 'TEST1', 'test@test.com');
INSERT INTO customers VALUES (2, 'Test2', 'TEST2', 'test@test.com');
INSERT INTO customers VALUES (3, 'Test2', 'TEST3', 'test@test.com');
----

- Add Signal Config to kafka Connector CR:

[source, yaml,indent=0]
----
oc apply -f https://raw.githubusercontent.com/aboucham/debezium/main/examples/kc-mysql-connector-signal.yaml
----

[source, yaml,indent=0]
----
    signal.kafka.topic: mysql.debezium_signal
    signal.kafka.bootstrap.servers: 'my-cluster-kafka-bootstrap:9092'
    signal.enabled.channels: 'source,kafka,jmx'
    signal.data.collection: inventory.debezium_signal
----


LOGS:

[source, yaml,indent=0]
----
dbz-mysql-connect-connect-746b688cbb-p2xvf dbz-mysql-connect-connect 2023-10-25 08:23:03,638 WARN [mysql-connector|task-0] [Consumer clientId=e79dab95-01b5-41d2-ad70-4662e56fa6a6, groupId=kafka-signal] Error while fetching metadata with correlation id 1 : {mysql.debezium_signal=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient) [debezium-mysqlconnector-mysql-SignalProcessor]
dbz-mysql-connect-connect-746b688cbb-p2xvf dbz-mysql-connect-connect 2023-10-25 08:23:03,638 INFO [mysql-connector|task-0] [Consumer clientId=e79dab95-01b5-41d2-ad70-4662e56fa6a6, groupId=kafka-signal] Cluster ID: DO1r2ddtQNKzZwOKSJnGhg (org.apache.kafka.clients.Metadata) [debezium-mysqlconnector-mysql-SignalProcessor]
dbz-mysql-connect-connect-746b688cbb-p2xvf dbz-mysql-connect-connect 2023-10-25 08:23:03,640 INFO [mysql-connector|task-0] [Consumer clientId=e79dab95-01b5-41d2-ad70-4662e56fa6a6, groupId=kafka-signal] Discovered group coordinator my-cluster-kafka-0.my-cluster-kafka-brokers.dbz-mysql.svc:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator) [debezium-mysqlconnector-mysql-SignalProcessor]
dbz-mysql-connect-connect-746b688cbb-p2xvf dbz-mysql-connect-connect 2023-10-25 08:23:03,645 INFO [mysql-connector|task-0] [Consumer clientId=e79dab95-01b5-41d2-ad70-4662e56fa6a6, groupId=kafka-signal] Found no committed offset for partition mysql.debezium_signal-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator) [debezium-mysqlconnector-mysql-SignalProcessor]
----

### Trigger Snapshot:

[source, yaml,indent=0]
----
#!/usr/bin/env bash
STRIMZI_IMAGE="registry.redhat.io/amq7/amq-streams-kafka-32-rhel8:2.2.0"
krun() { kubectl run krun-"$(date +%s)" -it --rm --restart="Never" --image="$STRIMZI_IMAGE" -- "$@"; }

krun bin/kafka-console-producer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic mysql.debezium_signal --property parse.key=true --property key.separator=:

mysql:{"type":"execute-snapshot","data": {"data-collections": ["inventory.customers"], "type": "INCREMENTAL"}}
----


- Remove Schema to the Kafka Connector `mysql-connector`

[source, yaml,indent=0]
----
    key.converter: 'org.apache.kafka.connect.json.JsonConverter'
    key.converter.schemas.enable: false
    value.converter: 'org.apache.kafka.connect.json.JsonConverter'
    value.converter.schemas.enable: false
----


- Remove Header to the Kafka Connector `mysql-connector`

[source, yaml,indent=0]
----
    transforms: unwrap
    transforms.unwrap.operation.header: true
    transforms.unwrap.drop.tombstones: false
    transforms.unwrap.type: io.debezium.transforms.ExtractNewRecordState

----

## Managing schema with ApiCurio Registry:

Add the following lines to the kafkaConnector CR for json converter:
[source, yaml,indent=0]
----
oc apply -f https://raw.githubusercontent.com/aboucham/debezium/main/examples/kc-mysql-connector-signal-sr-json.yaml
----
[source, yaml,indent=0]
----
    key.converter: io.apicurio.registry.utils.converter.ExtJsonConverter
    key.converter.apicurio.registry.url: http://apicurioregistry-psql-service:8080/apis/registry/v2
    key.converter.apicurio.registry.auto-register: true
    key.converter.apicurio.registry.find-latest: true
    value.converter: io.apicurio.registry.utils.converter.ExtJsonConverter
    value.converter.apicurio.registry.url: http://apicurioregistry-psql-service:8080/apis/registry/v2
    value.converter.apicurio.registry.auto-register: true
    value.converter.apicurio.registry.find-latest: true
----

Add the following lines to the kafkaConnector CR for Avro converter:
[source, yaml,indent=0]
----
oc apply -f https://raw.githubusercontent.com/aboucham/debezium/main/examples/kc-mysql-connector-signal-sr-avro.yaml
----
[source, yaml,indent=0]
----
    key.converter: io.apicurio.registry.utils.converter.AvroConverter
    key.converter.apicurio.registry.url: http://apicurioregistry-psql-service:8080/apis/registry/v2
    key.converter.apicurio.registry.auto-register: true
    key.converter.apicurio.registry.find-latest: true
    value.converter: io.apicurio.registry.utils.converter.AvroConverter
    value.converter.apicurio.registry.url: http://apicurioregistry-psql-service:8080/apis/registry/v2
    value.converter.apicurio.registry.auto-register: true
    value.converter.apicurio.registry.find-latest: true
----


## Camelk: Deploy kamelet

##### my topic to log. my-topic CR created in the same namespace as camel k

[source, yaml,indent=0]
----
oc create -f - <<EOF
kind: KameletBinding
apiVersion: camel.apache.org/v1alpha1
metadata:
  name: my-topic-to-log
spec:
  source:
    ref:
      kind: KafkaTopic
      apiVersion: kafka.strimzi.io/v1beta2
      name: mysql.inventory.customers
  sink:
    ref:
      apiVersion: camel.apache.org/v1alpha1
      kind: Kamelet
      name: log-sink
EOF
----

Different namespace:

[source, yaml,indent=0]
----
oc create -f - <<EOF
apiVersion: camel.apache.org/v1alpha1
kind: KameletBinding
metadata:
  name: kafkatopic-to-log
spec:
  source:
    ref:
      kind: Kamelet
      apiVersion: camel.apache.org/v1alpha1
      name: kafka-source
    properties:
      bootstrapServers: "my-cluster-kafka-bootstrap:9092"
      password: "testpassword"
      topic: "mysql.inventory.customers"
      user: "test-user"
      securityProtocol: "PLAINTEXT"
  sink:
    ref:
      apiVersion: camel.apache.org/v1alpha1
      kind: Kamelet
      name: log-sink
EOF
----
- Kamelet with ApiCurio Registry Jsonschema handling:

[source, yaml,indent=0]
----
oc apply -f https://raw.githubusercontent.com/aboucham/debezium/main/examples/kafka-apicurio-registry-json-source.kamelet.yaml
----

[source, yaml,indent=0]
----
oc create -f - <<EOF
apiVersion: camel.apache.org/v1alpha1
kind: KameletBinding
metadata:
  name: kafkatopic-apicurio-registry-log
spec:
  source:
    ref:
      kind: Kamelet
      apiVersion: camel.apache.org/v1alpha1
      name: kafka-apicurio-registry-source
    properties:
      bootstrapServers: "my-cluster-kafka-bootstrap:9092"
      topic: "mysql.inventory.customers"
      apicurioRegistryUrl: "http://apicurioregistry-psql-service:8080/apis/registry/v2"
  sink:
    ref:
      apiVersion: camel.apache.org/v1alpha1
      kind: Kamelet
      name: log-sink
EOF
----

- Kamelet with ApiCurio Registry Avro handling:

[source, yaml,indent=0]
----
oc apply -f https://raw.githubusercontent.com/aboucham/debezium/main/examples/kafka-apicurio-registry-avro-source.kamelet.yaml
----

[source, yaml,indent=0]
----
oc create -f - <<EOF
apiVersion: camel.apache.org/v1alpha1
kind: KameletBinding
metadata:
  name: kafka-ar-avro-source
spec:
  sink:
    ref:
      apiVersion: camel.apache.org/v1alpha1
      kind: Kamelet
      name: log-sink
  source:
    properties:
      apicurioRegistryUrl: 'http://apicurioregistry-psql-service:8080/apis/registry/v2'
      bootstrapServers: 'my-cluster-kafka-bootstrap:9092'
      topic: mysql.inventory.customers
    ref:
      apiVersion: camel.apache.org/v1alpha1
      kind: Kamelet
      name: kafka-apicurio-registry-avro-source
EOF
----


### Logs:

[source, yaml,indent=0]
----
kafka-ar-avro-source-58b5d84f44-2vkkr integration 2023-12-07 09:35:07,525 INFO  [info] (Camel (camel-1) thread #1 - KafkaConsumer[mysql.inventory.customers]) Exchange[ExchangePattern: InOnly, BodyType: org.apache.avro.generic.GenericData.Record, Body: {"before": null, "after": {"id": 11, "first_name": "Test11", "last_name": "TEST11", "email": "test@test.com"}, "source": {"version": "2.4.0.Final", "connector": "mysql", "name": "mysql", "ts_ms": 1701941706000, "snapshot": "false", "db": "inventory", "sequence": null, "table": "customers", "server_id": 1, "gtid": null, "file": "binlog.000002", "pos": 7087, "row": 0, "thread": 51, "query": null}, "op": "c", "ts_ms": 1701941706962, "transaction": null}]
----
